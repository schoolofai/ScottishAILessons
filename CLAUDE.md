# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Build and Development Commands

### Quick Start
```bash
# Official LangGraph
cd langgraph-agent && ./start.sh

# Aegra (self-hosted)
cd aegra-agent && ./start-aegra.sh
```

### LangGraph Agent

#### Development Setup
```bash
cd langgraph-agent
python3 -m venv ../venv
source ../venv/bin/activate
pip install -e . "langgraph-cli[inmem]"
```

#### Running Services
```bash
# Backend (port 2024)
langgraph dev

# Frontend (port 3000)
cd assistant-ui-frontend
npm install --legacy-peer-deps
npm run dev
```

#### Testing
```bash
cd langgraph-agent
pytest tests/
```

### Aegra Agent

#### Development Setup
```bash
cd aegra-agent
uv sync  # or: python3 -m venv .venv && source .venv/bin/activate && pip install -e .
docker compose up postgres -d
source .venv/bin/activate
python3 scripts/migrate.py upgrade
```

#### Running Services
```bash
# Backend (port 8000)
python3 run_server.py
# or with uvicorn:
uv run uvicorn src.agent_server.main:app --reload

# Frontend (port 3001)
cd ../assistant-ui-frontend
PORT=3001 npm run dev
```

#### Testing
```bash
cd aegra-agent
uv run pytest
uv run pytest tests/test_threads_history.py  # Specific test
uv run pytest --cov=src --cov-report=html  # With coverage
```

#### Database Management
```bash
# Apply migrations
python3 scripts/migrate.py upgrade

# Create new migration
python3 scripts/migrate.py revision -m "description"

# Check status
python3 scripts/migrate.py current
```

### Frontend (Shared)

#### Development Commands
```bash
cd assistant-ui-frontend

# Install dependencies
npm install --legacy-peer-deps

# Development server
npm run dev

# Build for production
npm run build

# Linting
npm run lint
```

#### Environment Configuration
The frontend uses different configurations based on the backend:
- `.env.local.langgraph` - LangGraph backend configuration (port 2024, frontend 3000)
- `.env.local.aegra` - Aegra backend configuration (port 8000, frontend 3001)
- `.env.local` - Active configuration (auto-generated by startup scripts)

## High-Level Architecture

### Dual Implementation Strategy
This repository provides two complete LangGraph implementations:

1. **Official LangGraph** (`langgraph-agent/`)
   - Managed cloud service with local development server
   - Uses LangGraph's built-in state management
   - Integrates with LangGraph Studio for visual debugging
   - Simple deployment with minimal configuration

2. **Aegra** (`aegra-agent/`)
   - Self-hosted alternative with PostgreSQL persistence
   - Full control over infrastructure and data
   - Agent Protocol compliant server wrapping LangGraph
   - Docker-based deployment with database migrations

### Shared Components

#### Frontend Architecture (`assistant-ui-frontend/`)
- Single React/Next.js application serves both backends
- Uses `@assistant-ui/react-langgraph` for LangGraph communication
- Environment-based backend switching via `.env.local` templates
- Responsive chat interface with streaming support
- Components: `MyAssistant.tsx` (main chat), `markdown-message.tsx` (rendering)

#### Agent Logic (`agents/`)
- `shared_chat_logic.py` - Core business logic used by both systems
- `langgraph_agent.py` - LangGraph wrapper using add_messages reducer
- `aegra_agent.py` - Aegra wrapper converting AIMessage to AIMessageChunk

### Backend Architecture Differences

#### Official LangGraph
- **Event Protocol**: Uses `event: values` for complete messages
- **Message Types**: Accepts both `AIMessage` and `AIMessageChunk`
- **State Management**: Built-in with LangGraph's StateGraph
- **Graph Definition**: Direct in `src/agent/graph.py`
- **Checkpointing**: Handled implicitly by LangGraph CLI in dev mode (stored in `.langraph_api` directory) - NO manual SqliteSaver setup needed

#### Aegra
- **Event Protocol**: Uses `event: messages` for streaming chunks
- **Message Types**: Requires `AIMessageChunk` for frontend compatibility
- **State Management**: PostgreSQL via LangGraph checkpoint system
- **Graph Loading**: Dynamic from `aegra.json` configuration
- **Database**: Hybrid approach - LangGraph handles state, SQLAlchemy tracks metadata

### Key Integration Points

#### LangGraph SDK Communication
Both systems use the same frontend SDK (`@langchain/langgraph-sdk`) but handle events differently:
- Official: Smart event selection based on message type
- Aegra: Consistent streaming-first approach with AIMessageChunk

#### Subgraph Streaming Configuration
**IMPORTANT**: The frontend must set `streamSubgraphs: true` in the client configuration to receive messages from subgraphs. This was discovered as a critical requirement for proper message streaming from the teaching subgraph. Without this setting, subgraph messages won't reach the frontend.

#### Structured Output Filtering with LangGraph Streaming
**CRITICAL GOTCHA**: When using `with_structured_output()` in LangGraph nodes, the JSON output streams to the frontend alongside regular content, creating duplicate/confusing UI messages.

**Problem**: LangGraph sends structured output as two separate streams:
1. `messages/metadata` event with `tags: ["json"]` and a specific `runId`
2. `messages/partial` events with the same `runId` containing JSON chunks token-by-token
3. Regular conversational content comes as separate `messages/partial` events with different `runId`s

**Solution**: Implement runId-based filtering in frontend streaming (see `assistant-ui-frontend/lib/chatApi.ts`):

```typescript
// Track runIds marked with "json" tag
const jsonRunIds = new Set<string>();

// 1. Detect metadata events and track JSON runIds
if (event.event === "messages/metadata" && event.data) {
  for (const [runId, runData] of Object.entries(event.data)) {
    if (runData.metadata?.tags?.includes("json")) {
      jsonRunIds.add(runId);
    }
  }
}

// 2. Filter partial events by runId
if (event.event === "messages/partial" && event.data?.[0]?.id) {
  if (jsonRunIds.has(event.data[0].id)) {
    continue; // Skip JSON chunks
  }
}
```

**Backend Setup**: Tag structured LLM calls with JSON:
```python
structured_llm = self.llm.with_structured_output(ResponseModel)
response = structured_llm.invoke(
    messages,
    config={"tags": ["json"], "run_name": "structured_evaluation"}
)
```

This approach is **generic** and works with any structured output schema, avoiding hardcoded content filtering.

#### AIMessage ID Duplication and Custom Tool Calls

**CRITICAL DISCOVERY**: When an LLM response is received, it already has a unique ID. Wrapping it in a new AIMessage creates a NEW unique ID, causing duplicate messages in the frontend.

**Problem**: 
```python
# DON'T DO THIS - causes duplication
message_obj = llm.invoke(...)  # Has ID: abc123
ai_message = AIMessage(content=message_obj.content, ...)  # NEW ID: xyz789
# Frontend sees both messages!
```

**Solution**:
```python
# Option 1: Use original message directly
return {"messages": [message_obj]}

# Option 2: Return both for different purposes
tool_message = AIMessage(content="", tool_calls=[...])  # Empty content
return {"messages": [message_obj, tool_message]}  # Content + tool trigger
```

**Custom Tool Calls for Assistant UI**:
You can create custom tool calls without LLM by wrapping in AIMessage:
```python
# Generate tool calls programmatically for Assistant UI components
tool_message = AIMessage(
    content="",  # Empty to avoid duplication
    tool_calls=[ToolCall(
        id="custom_id",
        name="ui_component_name",
        args={...}
    )]
)
```

This enables triggering Assistant UI's generative UI features even when the tool call isn't from a real LLM.

#### Authentication Systems
- Official LangGraph: Built-in authentication
- Aegra: Configurable (`AUTH_TYPE=noop` or `AUTH_TYPE=custom`)

#### Database Patterns
- Official: Managed by LangGraph platform
- Aegra: PostgreSQL with Alembic migrations + LangGraph checkpoint tables

## Git Submodule Management

Aegra is managed as a Git submodule (fork at `https://github.com/schoolofai/aegra.git`):

```bash
# Clone with submodules
git clone --recurse-submodules https://github.com/schoolofai/ScottishAILessons.git

# Update submodule
git submodule update --remote aegra-agent

# Work on Aegra customizations
cd aegra-agent
git add . && git commit -m "Changes" && git push
cd ..
git add aegra-agent && git commit -m "Update Aegra submodule"
```

## Project Structure

```
ScottishAILessons/
├── assistant-ui-frontend/       # Shared frontend
│   ├── components/             # React components
│   ├── app/                    # Next.js app router
│   └── .env.local.*           # Environment templates
├── langgraph-agent/            # Official LangGraph
│   ├── src/agent/graph.py     # Graph definition
│   ├── tests/                 # Unit tests
│   └── start.sh              # Startup script
├── aegra-agent/               # Self-hosted (submodule)
│   ├── src/agent_server/      # FastAPI server
│   ├── graphs/                # Agent definitions
│   ├── scripts/migrate.py    # Migration tool
│   └── start-aegra.sh       # Startup script
└── agents/                   # Shared logic
    ├── shared_chat_logic.py
    ├── langgraph_agent.py
    └── aegra_agent.py
```

## Testing Strategy

- Run `pytest` in langgraph-agent for official LangGraph tests
- Run `uv run pytest` in aegra-agent for Aegra tests
- No linting/type checking configured - focus on functional testing
- Both systems support hot-reload during development
- for mvp the test user login details are email - test@scottishailessons.com , password = red12345
- always use playwright mcp tool to mannually test after every code change