"""SOW upserter module - deterministic Python-based database persistence.

Handles upserting of authored SOWs to Appwrite database after agent completion.
Includes compression of large entries field to fit within Appwrite's 100k char limit.
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any

from .compression import compress_json_gzip_base64, get_compression_stats
from ..tools.sow_validator_tool import validate_sow_schema

logger = logging.getLogger(__name__)


async def upsert_sow_to_appwrite(
    sow_file_path: str,
    subject: str,
    level: str,
    course_id: str,
    execution_id: str,
    mcp_config_path: str
) -> str:
    """Upsert SOW to Appwrite deterministically.

    Process:
    1. Read authored_sow.json file
    2. Validate JSON structure (has metadata, entries)
    3. Transform to Appwrite schema:
       - Extract accessibility_notes from metadata (convert array to string)
       - Stringify entries array
       - Stringify metadata object
       - Add courseId, version="1", status="draft"
    4. Create document in default.Authored_SOW (ID auto-generated by Appwrite)
    5. Return document ID

    Args:
        sow_file_path: Path to authored_sow.json file
        subject: Subject slug (e.g., 'mathematics')
        level: Level slug (e.g., 'national-5')
        course_id: Validated courseId field value (e.g., 'course_c84474')
        execution_id: Execution timestamp ID
        mcp_config_path: Path to .mcp.json

    Returns:
        Appwrite document ID

    Raises:
        ValueError: If SOW file invalid
        FileNotFoundError: If SOW file missing
    """
    logger.info(f"üîÑ Starting SOW upsert to Appwrite")
    logger.info(f"  SOW file: {sow_file_path}")
    logger.info(f"  Subject: {subject}, Level: {level}")
    logger.info(f"  Course ID: {course_id}")

    # Step 1: Read and parse SOW
    sow_path = Path(sow_file_path)
    if not sow_path.exists():
        raise FileNotFoundError(
            f"SOW file not found: {sow_file_path}. "
            f"Agent may not have completed successfully."
        )

    try:
        with open(sow_path) as f:
            sow_data = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in SOW file: {e}")

    logger.info(f"‚úì SOW file loaded successfully")

    # Step 2: Validate structure
    if "metadata" not in sow_data:
        raise ValueError(
            "Invalid SOW: missing 'metadata' field. "
            "Check SOW author subagent output."
        )

    if "entries" not in sow_data:
        raise ValueError(
            "Invalid SOW: missing 'entries' field. "
            "Check SOW author subagent output."
        )

    if not isinstance(sow_data["entries"], list):
        raise ValueError(
            "Invalid SOW: 'entries' must be an array. "
            f"Got type: {type(sow_data['entries']).__name__}"
        )

    if len(sow_data["entries"]) == 0:
        raise ValueError(
            "Invalid SOW: 'entries' array is empty. "
            "SOW must have at least one lesson entry."
        )

    logger.info(f"‚úì SOW basic structure validated: {len(sow_data['entries'])} entries")

    # Step 2.5: Pydantic schema validation (comprehensive)
    logger.info("üîç Running Pydantic schema validation before database write...")
    sow_json_str = json.dumps(sow_data)
    validation_result = validate_sow_schema(sow_json_str)

    if not validation_result["valid"]:
        error_summary = validation_result["summary"]
        error_details = validation_result.get("errors", [])

        # Format error messages for display
        error_messages = []
        for error in error_details[:10]:  # Limit to 10 for readability
            location = error.get("location", "unknown")
            message = error.get("message", "validation error")
            error_messages.append(f"  - {location}: {message}")

        formatted_errors = "\n".join(error_messages)

        raise ValueError(
            f"‚ùå SOW failed Pydantic schema validation:\n"
            f"{error_summary}\n\n"
            f"Validation errors:\n{formatted_errors}\n\n"
            f"This indicates the SOW author/critic subagents produced invalid output. "
            f"Check workspace files for details."
        )

    logger.info(f"‚úì Pydantic validation passed: {validation_result['summary']}")

    # Step 3: Transform to Appwrite schema
    # Extract accessibility_notes from metadata
    accessibility_notes_raw = sow_data["metadata"].get("accessibility_notes", [])

    # Log raw accessibility_notes details for debugging
    logger.info(f"üîç Accessibility Notes Debug (raw):")
    logger.info(f"  Type: {type(accessibility_notes_raw).__name__}")
    if isinstance(accessibility_notes_raw, list):
        logger.info(f"  Array length: {len(accessibility_notes_raw)} items")
        logger.info(f"  Array content: {accessibility_notes_raw}")
    elif isinstance(accessibility_notes_raw, str):
        logger.info(f"  String length: {len(accessibility_notes_raw)} chars")
        logger.info(f"  String preview: {accessibility_notes_raw[:200]}...")
    else:
        logger.info(f"  Value: {accessibility_notes_raw}")

    # Convert to string for Appwrite (field type is string, not array)
    if isinstance(accessibility_notes_raw, list):
        # Join array items with newline + bullet points for readability
        accessibility_notes = "\n".join(f"‚Ä¢ {item}" for item in accessibility_notes_raw)
        logger.info(f"  ‚úì Converted array to string with bullet points")
    elif isinstance(accessibility_notes_raw, str):
        accessibility_notes = accessibility_notes_raw
        logger.info(f"  ‚úì Already a string, using as-is")
    else:
        # Fallback: convert to string
        accessibility_notes = str(accessibility_notes_raw)
        logger.info(f"  ‚ö†Ô∏è  Unexpected type, converted to string")

    logger.info(f"  Final string length: {len(accessibility_notes)} chars")
    logger.info(f"  Preview: {accessibility_notes[:200]}...")

    # Compress entries array using gzip+base64 (fits within Appwrite's 100k char limit)
    entries_compressed = compress_json_gzip_base64(sow_data["entries"])

    # Calculate compression stats for logging
    entries_stats = get_compression_stats(sow_data["entries"])

    # Stringify metadata object (keep uncompressed for admin readability)
    metadata_str = json.dumps(sow_data["metadata"])

    logger.info(f"‚úì Data transformation complete")
    logger.info(f"  Entries: {entries_stats['original']} ‚Üí {entries_stats['compressed']} chars ({entries_stats['savings']} reduction)")
    logger.info(f"  Metadata JSON size: {len(metadata_str)} chars")

    # Step 4: Build document
    document_data = {
        "courseId": course_id,
        "version": "1",  # Hardcoded for MVP
        "status": "draft",  # Hardcoded for MVP
        "entries": entries_compressed,  # Compressed using gzip+base64
        "metadata": metadata_str,
        "accessibility_notes": accessibility_notes
    }

    # Step 5: Use Appwrite's unique ID generator
    # Using "unique()" tells Appwrite to generate a collision-resistant ID
    # Format will be like: 68ebd6cf0003f478f102
    doc_id = "unique()"

    logger.info(f"‚úì Document prepared")
    logger.info(f"  Document ID: Appwrite auto-generated (unique())")
    logger.info(f"  Version: 1 (hardcoded)")
    logger.info(f"  Status: draft")

    # Log document data sizes for debugging
    logger.info(f"üîç Document Data Debug:")
    logger.info(f"  courseId: {document_data['courseId']}")
    logger.info(f"  version: {document_data['version']}")
    logger.info(f"  status: {document_data['status']}")
    logger.info(f"  entries size: {len(document_data['entries'])} chars (compressed)")
    logger.info(f"  metadata size: {len(document_data['metadata'])} chars")

    # Log accessibility_notes as it will be sent
    acc_notes_value = document_data['accessibility_notes']
    logger.info(f"  accessibility_notes type: {type(acc_notes_value).__name__}")
    if isinstance(acc_notes_value, str):
        logger.info(f"  accessibility_notes size: {len(acc_notes_value)} chars")
        logger.info(f"  accessibility_notes preview: {acc_notes_value[:300]}...")
    elif isinstance(acc_notes_value, list):
        logger.info(f"  accessibility_notes array length: {len(acc_notes_value)} items")
        logger.info(f"  accessibility_notes content: {acc_notes_value}")
        # This is what Appwrite will serialize
        serialized = json.dumps(acc_notes_value)
        logger.info(f"  accessibility_notes serialized size: {len(serialized)} chars")
    else:
        logger.info(f"  accessibility_notes value: {acc_notes_value}")

    # Step 6: Create document in Appwrite
    from .appwrite_mcp import create_appwrite_document

    try:
        result = await create_appwrite_document(
            database_id="default",
            collection_id="Authored_SOW",
            document_id=doc_id,
            data=document_data,
            permissions=["read(\"any\")"],
            mcp_config_path=mcp_config_path
        )

        logger.info(f"‚úÖ SOW upserted successfully to Appwrite")
        logger.info(f"  Document ID: {result['$id']}")
        logger.info(f"  Created at: {result.get('$createdAt', 'N/A')}")

        return result['$id']

    except Exception as e:
        logger.error(f"‚ùå Appwrite upsert failed!")
        logger.error(f"  Error type: {type(e).__name__}")
        logger.error(f"  Error message: {str(e)}")

        # Log which field might be causing the issue
        if "size" in str(e).lower() or "length" in str(e).lower() or "limit" in str(e).lower():
            logger.error(f"‚ö†Ô∏è  This looks like a field size issue!")
            logger.error(f"  Field sizes being sent:")
            logger.error(f"    - entries: {len(document_data['entries'])} chars (compressed)")
            logger.error(f"    - metadata: {len(document_data['metadata'])} chars")
            if isinstance(document_data['accessibility_notes'], str):
                logger.error(f"    - accessibility_notes: {len(document_data['accessibility_notes'])} chars")
            elif isinstance(document_data['accessibility_notes'], list):
                logger.error(f"    - accessibility_notes: {len(json.dumps(document_data['accessibility_notes']))} chars (serialized)")

        error_msg = (
            f"Failed to upsert SOW to Appwrite: {e}. "
            f"Check Appwrite connection, permissions, and field size limits."
        )
        raise ValueError(error_msg)
