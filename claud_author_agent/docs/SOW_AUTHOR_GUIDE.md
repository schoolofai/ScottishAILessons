# SOW Author System - Complete Guide

**Version**: 3.0 (Iterative Architecture)
**Last Updated**: 2026-01-20
**Testing Status**: ‚úÖ All 4 Phases Tested and Passed

## Table of Contents

1. [Overview](#overview)
2. [Phase Testing Results](#phase-testing-results-2026-01-20)
3. [Authoring Modes](#authoring-modes)
4. [Architecture](#architecture)
   - [Iterative Mode (Default)](#iterative-mode-architecture)
   - [Legacy Mode](#legacy-mode-architecture)
5. [Quick Start](#quick-start)
6. [Installation](#installation)
7. [Usage](#usage)
8. [Input/Output Specification](#inputoutput-specification)
9. [Subagent Prompts](#subagent-prompts)
10. [Token Optimization](#token-optimization)
11. [Troubleshooting](#troubleshooting)
12. [API Reference](#api-reference)

---

## Overview

The SOW (Scheme of Work) Author is an autonomous AI-powered system that generates complete course planning documents for Scottish secondary education. It transforms a simple course identifier into a comprehensive, pedagogically sound, curriculum-aligned scheme of work with an appropriate number of lesson entries (typically 10-20, agent-determined based on course complexity).

### Key Features

- ‚úÖ **Autonomous Authoring**: Generates complete SOW from courseId only
- ‚úÖ **Auto-Fetching**: Automatically retrieves subject/level from Appwrite database
- ‚úÖ **Belt-and-Braces Validation**: Two-tier validation (unified_critic + schema_critic)
- ‚úÖ **Pydantic Validation**: Fast, deterministic schema compliance (v2.0)
- ‚úÖ **On-Demand Research**: WebSearch/WebFetch for Scottish contexts and exemplars
- ‚úÖ **Cost Tracking**: Detailed token usage and cost metrics
- ‚úÖ **Workspace Isolation**: Each execution gets isolated filesystem
- ‚úÖ **Iterative Mode (v3.0)**: Lesson-by-lesson generation for better schema compliance

### What Gets Generated

A complete SOW includes:
- **Appropriate number of lesson entries** (typically 10-20, agent-determined) with full pedagogical design
- **5-card flow per teach lesson** (starter, explainer, modelling, guided_practice, exit_ticket) - simplified for iterative mode
- **9-card structure for mock_exam** (instructions + question cards covering all skills)
- **Assessment standard alignments** with enriched SQA descriptions
- **Accessibility profiles** (dyslexia-friendly, plain language, CEFR B1)
- **Scottish context integration** (local references, cultural relevance)
- **Policy compliance** (SQA calculator rules, assessment guidelines)
- **Coherence metadata** (sequencing notes, prerequisite tracking)

**Note**: Independent practice is handled by a **separate system** outside of SOW authoring.

---

## Phase Testing Results (2026-01-20)

All 4 phases of the iterative SOW author have been comprehensively tested with **Applications of Mathematics Higher** (`course_c84476`).

### Summary

| Phase | Description | Status | Duration |
|-------|-------------|--------|----------|
| **Phase 1** | Outline Generation | ‚úÖ PASSED | ~2 min |
| **Phase 2** | Per-Lesson Generation (19 lessons) | ‚úÖ PASSED | 63.9 min |
| **Phase 3** | Metadata Generation | ‚úÖ PASSED | 47.1 sec |
| **Phase 4** | Assembly & Appwrite Upsert | ‚úÖ PASSED | 0.7 sec |

### Phase 1: Outline Generation

| Metric | Value | Status |
|--------|-------|--------|
| **Unit Tests** | 32/32 passed | ‚úÖ |
| **Integration Tests** | 27/27 passed | ‚úÖ |
| **E2E Tests** | 19/19 passed | ‚úÖ |
| **Total Lessons** | 19 | ‚úÖ In sweet spot (15-20) |
| **Critic Score** | 0.76 | ‚úÖ PASS threshold (>0.7) |

### Phase 2: Per-Lesson Generation

| Metric | Value | Status |
|--------|-------|--------|
| **Total Lessons** | 19/19 generated | ‚úÖ |
| **Pass Rate** | 100% | ‚úÖ |
| **First-Attempt Pass** | 18/19 (94.7%) | ‚úÖ |
| **Required Revision** | 1 (Lesson 14) | ‚úÖ Critic loop worked |
| **Average Time/Lesson** | ~3.4 min | ‚úÖ |

### Phase 3: Metadata Generation

| Metric | Value | Status |
|--------|-------|--------|
| **Elapsed Time** | 47.1 seconds | ‚úÖ |
| **Policy Notes** | 5 items | ‚úÖ |
| **Sequencing Notes** | 7 items | ‚úÖ |
| **Accessibility Notes** | 6 items | ‚úÖ |
| **Engagement Notes** | 7 items | ‚úÖ |

### Phase 4: Assembly & Upsert

| Metric | Value | Status |
|--------|-------|--------|
| **Assembly Time** | 0.1 seconds | ‚úÖ |
| **Original Size** | 409,834 chars | - |
| **Compressed Size** | 127,501 chars | 68.9% reduction |
| **Storage Used** | Appwrite Storage Bucket | ‚úÖ (>100K limit) |
| **Final Document ID** | `696f676d27b78f0a71ae` | ‚úÖ |

**Test Workspaces** (preserved for reference):
- Phase 1: `workspace/20260119_220800/`
- Phase 2: `workspace/phase2_full_test_20260119_231045/`
- Phase 3: `workspace/phase3_test_20260120_093014/`
- Phase 4: `workspace/phase4_test_20260120_*/`

---

## Authoring Modes

**v3.0** introduces two SOW authoring modes. The **iterative mode** (default) generates lessons one at a time for better schema compliance, while the **legacy mode** generates the entire SOW in a single monolithic pass.

### Mode Comparison

| Aspect | Iterative (Default) | Legacy |
|--------|---------------------|--------|
| **CLI Flag** | `--iterative` (or omit) | `--legacy` |
| **Class** | `IterativeSOWAuthor` | `SOWAuthorClaudeAgent` |
| **Generation** | Lesson-by-lesson (~4K tokens each) | Monolithic (~50K+ tokens) |
| **Schema Compliance** | ‚úÖ Better (small scope) | ‚ö†Ô∏è May drift |
| **Cross-Lesson Coherence** | ‚úÖ Explicit validation | Implicit in prompt |
| **Debugging** | ‚úÖ Per-lesson workspace files | Single authored_sow.json |
| **Web Research** | ‚úÖ Per-lesson WebSearch/WebFetch | ‚úÖ Full document research |

### When to Use Each Mode

**Use Iterative Mode (Default)** when:
- Creating new courses
- Schema compliance issues have occurred with legacy mode
- Need fine-grained debugging of lesson generation
- Lessons require individualized web research

**Use Legacy Mode** when:
- Iterative mode encounters issues
- Backward compatibility is needed
- Existing workflows depend on legacy behavior

### CLI Usage

```bash
# Iterative mode (default)
python -m src.sow_author_cli --courseId course_c84474

# Explicit iterative mode
python -m src.sow_author_cli --courseId course_c84474 --iterative

# Legacy mode
python -m src.sow_author_cli --courseId course_c84474 --legacy

# Via DevOps pipeline
./devops/pipeline.sh lessons --subject mathematics --level national_5          # iterative
./devops/pipeline.sh lessons --subject physics --level higher --legacy          # legacy
```

---

## Architecture

The SOW Author system supports two architectures depending on the authoring mode selected.

### Iterative Mode Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ITERATIVE SOW AUTHOR PIPELINE (v3.0 - Default)                  ‚îÇ
‚îÇ                    Lesson-by-Lesson with Claude Agent SDK                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Course_outcomes.json     ‚îÇ
                    ‚îÇ  (SQA curriculum data from ‚îÇ
                    ‚îÇ   Appwrite, Python extract)‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó ‚îÇ
‚îÇ  ‚ïë         PHASE 1: OUTLINE GENERATION (Claude Agent SDK)               ‚ïë ‚îÇ
‚îÇ  ‚ïë                                                                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   Subagent: outline_author + outline_critic (PASS/REVISE loop)      ‚ïë ‚îÇ
‚îÇ  ‚ïë   Prompt: src/prompts/outline_author_prompt.md                       ‚ïë ‚îÇ
‚îÇ  ‚ïë   Output: lesson_outline.json                                        ‚ïë ‚îÇ
‚îÇ  ‚ïë                                                                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   Creates: Lesson sequence (teach + mock_exam only), standards map  ‚ïë ‚îÇ
‚îÇ  ‚ïë   Validated: Pydantic + Critic score >= 0.7                         ‚ïë ‚îÇ
‚îÇ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó ‚îÇ
‚îÇ  ‚ïë       PHASE 2: LESSON GENERATION (Claude Agent SDK, Loop N times)    ‚ïë ‚îÇ
‚îÇ  ‚ïë                                                                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   FOR each lesson in outline (orchestrated by Python):              ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îÇ  Subagent: lesson_author + lesson_critic (PASS/REVISE loop)   ‚îÇ ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îÇ  Prompt: src/prompts/lesson_entry_prompt.md                    ‚îÇ ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îÇ  Context: Course_outcomes + outline + previous_lessons         ‚îÇ ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îÇ  Output: lesson_{N}.json (~4K tokens each)                     ‚îÇ ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îÇ                                                                ‚îÇ ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îÇ  ‚úÖ Pydantic validation via structured output                  ‚îÇ ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îÇ  ‚úÖ Critic loop with 5-dimension scoring (>0.7 = PASS)         ‚îÇ ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îÇ  ‚úÖ Previous lessons provide coherence context                  ‚îÇ ‚ïë ‚îÇ
‚îÇ  ‚ïë   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚ïë ‚îÇ
‚îÇ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó ‚îÇ
‚îÇ  ‚ïë         PHASE 3: METADATA GENERATION (Claude Agent SDK)              ‚ïë ‚îÇ
‚îÇ  ‚ïë                                                                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   Subagent: metadata_author (no critic - summarization task)        ‚ïë ‚îÇ
‚îÇ  ‚ïë   Prompt: src/prompts/metadata_author_prompt.md                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   Output: metadata.json                                              ‚ïë ‚îÇ
‚îÇ  ‚ïë                                                                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   Creates: Coherence notes, accessibility notes, engagement notes    ‚ïë ‚îÇ
‚îÇ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó ‚îÇ
‚îÇ  ‚ïë              PHASE 4: ASSEMBLY (Pure Python, No LLM)                 ‚ïë ‚îÇ
‚îÇ  ‚ïë                                                                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   Module: src/utils/sow_assembler.py                                 ‚ïë ‚îÇ
‚îÇ  ‚ïë                                                                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   - Combines lesson_01..N.json + metadata.json                      ‚ïë ‚îÇ
‚îÇ  ‚ïë   - Cross-lesson validation (order, mock_exam count)                ‚ïë ‚îÇ
‚îÇ  ‚ïë   - Final AuthoredSOWIterative Pydantic validation                  ‚ïë ‚îÇ
‚îÇ  ‚ïë   - Compression (gzip+base64) for entries                           ‚ïë ‚îÇ
‚îÇ  ‚ïë   - Storage Bucket fallback if > 100K chars                         ‚ïë ‚îÇ
‚îÇ  ‚ïë   - Upsert to Appwrite                                              ‚ïë ‚îÇ
‚îÇ  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Iterative Mode Files

| File | Purpose |
|------|---------|
| `src/iterative_sow_author.py` | Main orchestrator (uses Claude Agent SDK) |
| **Prompts** | |
| `src/prompts/outline_author_prompt.md` | Outline generation prompt |
| `src/prompts/outline_critic_prompt.md` | Outline critic prompt (PASS/REVISE) |
| `src/prompts/lesson_entry_prompt.md` | Single lesson generation prompt |
| `src/prompts/lesson_critic_prompt.md` | Lesson critic prompt (5-dimension scoring) |
| `src/prompts/metadata_author_prompt.md` | Metadata generation prompt |
| **Schema Models** | |
| `src/tools/sow_schema_models.py` | Pydantic models (LessonOutline, SOWEntry, Metadata, AuthoredSOW) |
| `src/tools/critic_schema_models.py` | Pydantic models for critic results |
| `src/utils/minimal_schemas.py` | Minimal JSON schemas for SDK structured output |
| **Assembly & Storage** | |
| `src/utils/sow_assembler.py` | Pure Python cross-lesson validation & assembly |
| `src/utils/sow_upserter.py` | Appwrite upsert with compression & storage bucket |
| `src/utils/entry_trimmer.py` | Non-essential field trimming for size reduction |
| `src/utils/storage_helpers.py` | Appwrite Storage bucket upload/download |

### Legacy Mode Architecture

### Pipeline Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SOW AUTHOR PIPELINE                       ‚îÇ
‚îÇ                        (3 stages)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 0: Pre-Processing (Python Utilities)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  1. Course Data Extractor                                    ‚îÇ
‚îÇ     Input:  courseId                                         ‚îÇ
‚îÇ     Output: /workspace/Course_data.txt                       ‚îÇ
‚îÇ     Purpose: Extract official SQA data from Appwrite         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  2. Pydantic Validator Setup (v2.0 TOKEN OPTIMIZATION)      ‚îÇ
‚îÇ     Tool:   mcp__validator__validate_sow_schema              ‚îÇ
‚îÇ     Source: src/tools/sow_validator_tool.py                  ‚îÇ
‚îÇ     Replaces: 1265-line SOW_Schema.md file                   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 1: SOW Author (LLM Subagent)                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  @sow_author subagent                                        ‚îÇ
‚îÇ  Prompt: src/prompts/sow_author_prompt.md (588 lines)       ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Input:                                                       ‚îÇ
‚îÇ    ‚Ä¢ /workspace/Course_data.txt                              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Tools:                                                       ‚îÇ
‚îÇ    ‚Ä¢ Read, Write, Edit, Glob, Grep, TodoWrite, Task         ‚îÇ
‚îÇ    ‚Ä¢ WebSearch, WebFetch (on-demand research)                ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Output:                                                      ‚îÇ
‚îÇ    ‚Ä¢ /workspace/authored_sow.json                            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Strategy:                                                    ‚îÇ
‚îÇ    ‚Ä¢ Schema-driven 10-step process                           ‚îÇ
‚îÇ    ‚Ä¢ Incremental writing (prevents token limits)             ‚îÇ
‚îÇ    ‚Ä¢ On-demand research (Scottish contexts, misconceptions)  ‚îÇ
‚îÇ    ‚Ä¢ Enriched format (objects not bare strings)              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 2: Unified Critic (BELT - LLM Subagent)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  @unified_critic subagent                                    ‚îÇ
‚îÇ  Prompt: src/prompts/unified_critic_prompt.md (1142 lines)  ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Input:                                                       ‚îÇ
‚îÇ    ‚Ä¢ /workspace/Course_data.txt                              ‚îÇ
‚îÇ    ‚Ä¢ /workspace/authored_sow.json                            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Validation:                                                  ‚îÇ
‚îÇ    1. Schema Gate (blocking) - enriched format, CFU          ‚îÇ
‚îÇ    2. Five Dimensions:                                       ‚îÇ
‚îÇ       - Coverage (standards alignment)                       ‚îÇ
‚îÇ       - Sequencing (pedagogical progression)                 ‚îÇ
‚îÇ       - Policy (SQA compliance)                              ‚îÇ
‚îÇ       - Accessibility (inclusive design)                     ‚îÇ
‚îÇ       - Authenticity (Scottish contexts)                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Output:                                                      ‚îÇ
‚îÇ    ‚Ä¢ /workspace/sow_critic_result.json                       ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Retry Logic:                                                 ‚îÇ
‚îÇ    ‚Ä¢ Max 3 attempts                                          ‚îÇ
‚îÇ    ‚Ä¢ Feedback loop: critic ‚Üí author ‚Üí critic                ‚îÇ
‚îÇ    ‚Ä¢ Blocks on schema_gate failures                         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 3: Schema Critic (BRACES - LLM + Pydantic Tool)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  @schema_critic subagent (v2.0 Pydantic-based)               ‚îÇ
‚îÇ  Prompt: src/prompts/schema_critic_prompt.md (322 lines)    ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Input:                                                       ‚îÇ
‚îÇ    ‚Ä¢ /workspace/authored_sow.json                            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Tool:                                                        ‚îÇ
‚îÇ    ‚Ä¢ mcp__validator__validate_sow_schema                     ‚îÇ
‚îÇ    ‚Ä¢ Pydantic models (390 lines)                             ‚îÇ
‚îÇ    ‚Ä¢ 8 custom validators                                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Validation Checks:                                           ‚îÇ
‚îÇ    ‚úì Enriched format (entry + card level)                   ‚îÇ
‚îÇ    ‚úì CFU strategy specificity                               ‚îÇ
‚îÇ    ‚úì Metadata completeness                                   ‚îÇ
‚îÇ    ‚úì Card structure (6-12 cards, required fields)           ‚îÇ
‚îÇ    ‚úì Timing alignment (¬±2 min tolerance)                    ‚îÇ
‚îÇ    ‚úì Entry sequencing (1, 2, 3...)                          ‚îÇ
‚îÇ    ‚úì Rubric points validation                               ‚îÇ
‚îÇ    ‚úì Teach-revision pairing (1:1 within 3 entries)          ‚îÇ
‚îÇ    ‚úì Course requirements (‚â•1 independent, =1 mock)          ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Output:                                                      ‚îÇ
‚îÇ    ‚Ä¢ /workspace/schema_validation_result.json                ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  v2.0 Improvement:                                            ‚îÇ
‚îÇ    ‚Ä¢ Execution: 30+ seconds ‚Üí 5-10 seconds                   ‚îÇ
‚îÇ    ‚Ä¢ Token savings: ~13-16K per execution                    ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ STAGE 4: Post-Processing (Python Utilities)                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  SOW Upserter (with Pydantic Pre-Flight Validation)         ‚îÇ
‚îÇ    Input:  /workspace/authored_sow.json                      ‚îÇ
‚îÇ    Validation: Pydantic schema check before DB write         ‚îÇ
‚îÇ    Output: Appwrite document in default.Authored_SOW         ‚îÇ
‚îÇ    Purpose: Final validation + save to production database   ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  Fail-Fast Protection:                                       ‚îÇ
‚îÇ    - Validates SOW with Pydantic before database write       ‚îÇ
‚îÇ    - Prevents invalid data from reaching production          ‚îÇ
‚îÇ    - Provides exact error locations if validation fails      ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Belt-and-Braces Strategy

**BELT (Unified Critic)**:
- **Schema Gate** (v3.0): Uses Pydantic validation tool for early schema checking
- Catches schema issues EARLY during pedagogical review (blocking gate)
- Comprehensive 5-dimension pedagogical validation (Coverage, Sequencing, Policy, Accessibility, Authenticity)
- Iterative feedback loop with author

**BRACES (Schema Critic)**:
- Final schema-only validation using same Pydantic tool (defensive double-check)
- Deterministic, fast (5-10 seconds)
- Catches ANY remaining schema violations
- Zero tolerance (any error = FAIL)

**Together**: Belt-and-braces with shared Pydantic validation ensures consistent schema compliance across both stages

---

## Quick Start

### Minimal Example (Interactive Mode)

```bash
cd claud_author_agent
python -m src.sow_author_cli
```

You'll be prompted for the courseId. The system will:
1. Auto-fetch subject/level from Appwrite
2. Generate complete SOW (appropriate number of lessons)
3. Validate with belt-and-braces strategy (unified_critic + schema_critic)
4. Final Pydantic validation before database write
5. Save to Appwrite database

### Command-Line Example

```bash
# Simple courseId input
python -m src.sow_author_cli --courseId course_c84474

# With custom configuration
python -m src.sow_author_cli \
  --courseId course_c84474 \
  --log-level DEBUG \
  --no-persist-workspace
```

### JSON File Example

Create `input.json`:
```json
{
  "courseId": "course_c84474"
}
```

Run:
```bash
python -m src.sow_author_cli --input input.json
```

---

## Installation

### Prerequisites

1. **Python 3.11+**
2. **Appwrite Database** (with courses collection)
3. **Claude API Key** (Anthropic)
4. **MCP Configuration** (.mcp.json)

### Setup Steps

```bash
# 1. Navigate to project directory
cd claud_author_agent

# 2. Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -e .
pip install claude-agent-sdk mcp pydantic

# 4. Configure MCP (Appwrite connection)
cp .mcp.json.example .mcp.json
# Edit .mcp.json with your Appwrite credentials

# 5. Set Claude API key
export ANTHROPIC_API_KEY="your-api-key-here"

# 6. Test installation
python -m src.sow_author_cli --help
```

### MCP Configuration (.mcp.json)

```json
{
  "mcpServers": {
    "appwrite": {
      "command": "npx",
      "args": [
        "-y",
        "@niladribose/mcp-appwrite-server"
      ],
      "env": {
        "APPWRITE_ENDPOINT": "https://cloud.appwrite.io/v1",
        "APPWRITE_PROJECT_ID": "your-project-id",
        "APPWRITE_API_KEY": "your-api-key"
      }
    }
  }
}
```

---

## Usage

### CLI Options

```bash
python -m src.sow_author_cli [OPTIONS]

OPTIONS:
  # Input Methods (mutually exclusive)
  --input JSON_FILE          Load courseId from JSON file
  --courseId COURSE_ID       Provide courseId directly
  # (no options)              Interactive mode

  # Authoring Mode (v3.0)
  --iterative                Use iterative lesson-by-lesson mode (default)
  --legacy                   Use legacy monolithic mode

  # Configuration
  --mcp-config PATH          Path to MCP config (default: .mcp.json)
  --no-persist-workspace     Delete workspace after execution
  --log-level LEVEL          DEBUG|INFO|WARNING|ERROR (default: INFO)

  # Help
  --help                     Show help message
```

### Programmatic API

```python
import asyncio
from src.sow_author_claude_client import SOWAuthorClaudeAgent

async def generate_sow():
    # Initialize agent
    agent = SOWAuthorClaudeAgent(
        mcp_config_path=".mcp.json",
        persist_workspace=True,
        log_level="INFO"
    )

    # Execute (subject/level auto-fetched)
    result = await agent.execute(courseId="course_c84474")

    if result["success"]:
        print(f"‚úÖ SOW created: {result['appwrite_document_id']}")
        print(f"üìä Cost: ${result['metrics']['total_cost_usd']:.4f}")
    else:
        print(f"‚ùå Failed: {result['error']}")

    return result

# Run
result = asyncio.run(generate_sow())
```

---

## Input/Output Specification

### Input Format

**Minimal Input** (courseId only):
```json
{
  "courseId": "course_c84474"
}
```

**What the system auto-fetches from Appwrite**:
- Subject (e.g., "mathematics")
- Level (e.g., "national-5")
- Course title
- SQA course code
- Units, outcomes, assessment standards

### Output Format

**Success Response**:
```json
{
  "success": true,
  "execution_id": "20251029_143045",
  "workspace_path": "/tmp/sow_author_20251029_143045",
  "appwrite_document_id": "68f616168886c3362749",
  "metrics": {
    "total_tokens": 85432,
    "total_cost_usd": 1.2456,
    "execution_time_seconds": 245.3,
    "subagent_metrics": {
      "sow_author": {"tokens": 42315, "cost_usd": 0.6234},
      "unified_critic": {"tokens": 28456, "cost_usd": 0.4187},
      "schema_critic": {"tokens": 14661, "cost_usd": 0.2035}
    }
  }
}
```

**Failure Response**:
```json
{
  "success": false,
  "execution_id": "20251029_143045",
  "error": "Schema validation failed after 3 retries",
  "workspace_path": "/tmp/sow_author_20251029_143045",
  "metrics": {
    "total_tokens": 45123,
    "total_cost_usd": 0.6234
  }
}
```

### Generated SOW Structure

The `authored_sow.json` file contains:

```json
{
  "$id": "68f616168886c3362749",
  "courseId": "course_c84474",
  "version": "1",
  "status": "published",

  "metadata": {
    "coherence": {
      "policy_notes": ["Non-calculator sections first", "..."],
      "sequencing_notes": ["Fractions ‚Üí Decimals ‚Üí Percentages", "..."]
    },
    "accessibility_notes": ["Dyslexia-friendly fonts", "..."],
    "engagement_notes": ["Scottish shop prices", "..."],
    "weeks": 10,
    "periods_per_week": 4
  },

  "entries": [
    {
      "order": 1,
      "label": "Fractions: Introduction and Real-World Contexts",
      "lesson_type": "teach",

      "coherence": {
        "block_name": "Fractions Unit",
        "block_index": "1.1",
        "prerequisites": []
      },

      "policy": {
        "calculator_section": "non_calc",
        "assessment_notes": "Mental fraction work"
      },

      "engagement_tags": ["scottish_contexts", "real_world"],
      "outcomeRefs": ["O1"],

      "assessmentStandardRefs": [
        {
          "code": "AS1.2",
          "description": "Add and subtract fractions by expressing them with a common denominator and then operating on the numerators",
          "outcome": "O1"
        }
      ],

      "lesson_plan": {
        "summary": "Students explore fractions in Scottish contexts...",

        "card_structure": [
          {
            "card_number": 1,
            "card_type": "starter",
            "title": "Real-World Fractions",
            "purpose": "Activate prior knowledge",
            "standards_addressed": [
              {
                "code": "AS1.2",
                "description": "Add and subtract fractions...",
                "outcome": "O1"
              }
            ],
            "pedagogical_approach": "Show images of fractions in daily life",
            "key_concepts": ["numerator", "denominator"],
            "cfu_strategy": "MCQ: Where do you see fractions? A) supermarket B) bus schedule",
            "estimated_minutes": 5
          }
          // ... 5-11 more cards
        ],

        "lesson_flow_summary": "Starter ‚Üí Explainer ‚Üí Modelling ‚Üí Practice ‚Üí Exit",
        "multi_standard_integration_strategy": "Fractions + percentages connection",
        "misconceptions_embedded_in_cards": ["misc_card_2", "misc_card_5"],
        "assessment_progression": "Formative ‚Üí Summative"
      },

      "accessibility_profile": {
        "dyslexia_friendly": true,
        "plain_language_level": "CEFR_B1",
        "extra_time": true,
        "extra_time_percentage": 25,
        "key_terms_simplified": ["numerator", "denominator"],
        "visual_support_strategy": "Fraction diagrams"
      },

      "estMinutes": 50,
      "lesson_instruction": "Introduce fractions using Scottish shop prices..."
    }
    // ... 7-24 more entries
  ],

  "accessibility_notes": "All lessons use dyslexia-friendly fonts, plain language (CEFR B1), and provide 25% extra time..."
}
```

---

## Subagent Prompts

### 1. SOW Author Prompt
**Location**: `src/prompts/sow_author_prompt.md` (588 lines)

**Purpose**: Create complete SOW from course data

**Key Sections**:
- **10-Step Process**: Incremental authoring strategy
- **Enriched Format Requirements**: Objects not bare strings
- **CFU Specificity Rules**: No generic "ask questions"
- **Scottish Context Integration**: Local references, cultural relevance
- **Research Guidelines**: When/how to use WebSearch/WebFetch
- **Incremental Writing Strategy**: Prevents 32K token limits

**Example CFU Strategies** (from prompt):
- ‚úÖ **GOOD**: "MCQ: Which fraction equals 25%? A) 1/4 B) 1/2 C) 1/3 D) 2/4"
- ‚úÖ **GOOD**: "Numeric: A box costs ¬£12 and is reduced by 1/3. What's the discount?"
- ‚ùå **BAD**: "Ask questions"
- ‚ùå **BAD**: "Check understanding"

---

### 2. Unified Critic Prompt
**Location**: `src/prompts/unified_critic_prompt.md` (1142 lines)

**Purpose**: Comprehensive validation (schema gate + 5 dimensions)

**Schema Gate** (blocking validation):
1. Enriched format at entry level (code/description/outcome objects)
2. Enriched format at card level (standards_addressed objects)
3. CFU strategies are specific (not generic)

**Five Dimensions** (pedagogical scoring):
1. **Coverage** (0.0-1.0): Standards alignment completeness
2. **Sequencing** (0.0-1.0): Pedagogical progression quality
3. **Policy** (0.0-1.0): SQA compliance (calculator rules, assessment)
4. **Accessibility** (0.0-1.0): Inclusive design (dyslexia, plain language)
5. **Authenticity** (0.0-1.0): Scottish contexts integration

**Pass Criteria**:
- `schema_gate.pass == true` (blocking)
- `overall_score >= 0.8` (all dimensions ‚â• 0.7)

**Retry Logic**:
- Max 3 attempts
- Feedback loop: critic ‚Üí author ‚Üí critic
- Blocks immediately on schema_gate failures

---

### 3. Schema Critic Prompt
**Location**: `src/prompts/schema_critic_prompt.md` (322 lines, v2.0 simplified)

**Purpose**: Final schema-only validation using Pydantic tool

**v2.0 Changes**:
- ‚ùå No manual validation logic
- ‚ùå No reading 1265-line schema file
- ‚úÖ Single tool call: `mcp__validator__validate_sow_schema`
- ‚úÖ Deterministic Pydantic validation
- ‚úÖ Token savings: ~13-16K per execution

**Validation Process**:
1. Read `/workspace/authored_sow.json`
2. Call Pydantic validation tool
3. Transform tool output to expected format
4. Write `/workspace/schema_validation_result.json`

**What Gets Validated** (by Pydantic tool):
- Enriched format compliance (entry + card level)
- CFU strategy specificity (rejects generic phrases)
- Complete metadata (all required fields, non-empty arrays)
- Card structure integrity (6-12 cards, required fields)
- Card timing alignment (sum matches estMinutes ¬±2 min)
- Entry order sequencing (1, 2, 3...)
- Rubric points validation (criteria sum to total_points)
- Teach-revision pairing (1:1 within 3 entries)
- Course requirements (‚â•1 independent_practice, exactly 1 mock_assessment)

---

### Iterative Mode Prompts (v3.0)

The iterative mode uses three specialized prompts for each phase of generation:

#### 4. Outline Author Prompt
**Location**: `src/prompts/outline_author_prompt.md` (~180 lines)

**Purpose**: Generate the lesson sequence outline before detailed generation

**Key Responsibilities**:
- Analyze Course_data.txt for standards and units
- Plan lesson sequence (10-20 lessons typically)
- Establish teach-revision pairing (1:1 within 3 entries)
- Map assessment standards to lessons
- Determine lesson types (teach, revision, independent_practice, mock_assessment)

**Output**: `lesson_outline.json` with `LessonOutline` schema

---

#### 5. Lesson Entry Prompt
**Location**: `src/prompts/lesson_entry_prompt.md` (~241 lines)

**Purpose**: Generate a single lesson entry with full pedagogical detail

**Key Responsibilities**:
- Read outline entry for current lesson context
- Use previous lessons for coherence
- Generate 6-12 cards with specific CFU strategies
- Apply WebSearch/WebFetch for Scottish context and misconceptions
- Ensure enriched format (objects not bare strings)

**Context Files Available**:
- `Course_data.txt` - SQA curriculum data
- `lesson_outline.json` - Full outline
- `current_outline.json` - Current lesson's outline entry
- `previous_lessons.json` - Previously generated lessons

**Output**: `lesson_{N}.json` with `SOWEntry` schema

---

#### 6. Metadata Author Prompt
**Location**: `src/prompts/metadata_author_prompt.md` (~198 lines)

**Purpose**: Generate course-level metadata after all lessons are complete

**Key Responsibilities**:
- Summarize coherence across all lessons
- Document accessibility strategies
- Capture engagement approaches
- Calculate weeks and periods per week

**Output**: `metadata.json` with `SOWMetadata` schema

---

## Token Optimization

### v2.0 Improvements (Pydantic-Based Validation)

**Before (v1.0)**:
- Schema file copy: 1265 lines √ó ~6 tokens/line = ~7,590 tokens
- Schema_critic prompt: 737 lines √ó ~6 tokens/line = ~4,422 tokens
- Manual validation logic: ~30 seconds execution
- **Total overhead**: ~12,000 tokens per SOW execution

**After (v2.0)**:
- Schema file copy: **ELIMINATED** (0 tokens)
- Schema_critic prompt: 322 lines √ó ~6 tokens/line = ~1,932 tokens
- Pydantic validation: ~5-10 seconds execution
- **Total overhead**: ~2,000 tokens per SOW execution

**Token Savings**: ~10,000 tokens per SOW execution
**Cost Savings**: ~$0.30-0.40 per SOW (at $30/million tokens)
**Time Savings**: ~20-25 seconds per validation

### Cost Breakdown (Typical SOW)

```
Subagent Costs (Sonnet 4.5 @ $3 input / $15 output per million tokens):
‚îú‚îÄ SOW Author:        ~40K tokens  ‚Üí  $0.60
‚îú‚îÄ Unified Critic:    ~25K tokens  ‚Üí  $0.38
‚îú‚îÄ Schema Critic:     ~15K tokens  ‚Üí  $0.23 (v2.0 optimized)
‚îî‚îÄ Total:             ~80K tokens  ‚Üí  $1.20

v1.0 Total:           ~95K tokens  ‚Üí  $1.43
v2.0 Total:           ~80K tokens  ‚Üí  $1.20
Savings:              ~15K tokens  ‚Üí  $0.23 per SOW
```

**Annual Savings** (100 SOWs/year):
- Token savings: 1.5M tokens
- Cost savings: ~$23

---

## Troubleshooting

### Common Issues

#### 1. "Course not found in database"

**Cause**: Invalid courseId or missing from Appwrite
**Solution**:
```bash
# Verify courseId exists in default.courses collection
# Check MCP connection to Appwrite in .mcp.json
```

#### 2. "Schema validation failed after 3 retries"

**Cause**: Author consistently produces invalid schema
**Solution**:
```bash
# Check workspace for detailed errors
# Iterative mode:
cd claud_author_agent/workspace/<execution_id>
cat lesson_XX_critic.json  # Check specific lesson critic result

# Legacy mode:
cd /tmp/sow_author_<execution_id>
cat schema_validation_result.json

# Common fixes:
# - Ensure Course_outcomes.json has complete SQA descriptions
# - Check for generic CFU strategies ("ask questions")
# - Verify enriched format (objects not bare strings)
```

#### 3. "MCP server connection failed"

**Cause**: Invalid Appwrite credentials or network issues
**Solution**:
```bash
# Test MCP connection manually
cat .mcp.json  # Verify credentials
# Check Appwrite endpoint and API key
# Ensure network access to Appwrite
```

#### 4. "Out of tokens / Context limit exceeded"

**Cause**: SOW too large for single generation
**Solution**:
```python
# The system uses incremental writing to prevent this
# If still occurs:
# - Agent will automatically determine appropriate entry count
# - Split into multiple smaller courses if needed
# - Increase max_output_tokens (env var)
# - Note: Entry count is flexible (no hardcoded max)
```

#### 5. "Pydantic validation errors"

**Cause**: SOW structure doesn't match schema models
**Solution**:
```bash
# Test validation standalone
cd claud_author_agent
python3 src/tools/sow_validator_tool.py example_sow.json

# Check specific error locations in output
# Update author prompt if systematic issues found
```

#### 6. "Entries too large for Appwrite" / "Storage Bucket upload failed"

**Cause**: Compressed entries exceed 100K character limit
**Solution**:
```bash
# The system automatically handles this:
# 1. Entry trimming removes non-essential fields
# 2. If still >100K, uploads to Appwrite Storage bucket
# 3. stores "storage:<file_id>" reference in entries field

# To verify storage bucket exists:
# Check Appwrite Console ‚Üí Storage ‚Üí authored_sow_entries bucket

# To manually check entry size:
cd claud_author_agent/workspace/<execution_id>
python3 -c "
import json
from pathlib import Path
data = json.loads(Path('authored_sow.json').read_text())
entries_json = json.dumps(data['entries'])
print(f'Entries size: {len(entries_json):,} chars')
"
```

### Debug Mode

Enable verbose logging:
```bash
python -m src.sow_author_cli \
  --courseId course_c84474 \
  --log-level DEBUG
```

**Note**: Remove `--no-persist-workspace` to keep workspace for inspection.

Inspect workspace files:
```bash
# Iterative mode (default):
cd claud_author_agent/workspace/<execution_id>
ls -la

# Key files (iterative):
# - Course_outcomes.json (extracted SQA data)
# - lesson_outline.json (Phase 1 output)
# - outline_critic_result.json (Phase 1 critic)
# - lesson_01.json ... lesson_N.json (Phase 2 outputs)
# - lesson_XX_critic.json (Phase 2 critic per lesson)
# - metadata.json (Phase 3 output)
# - authored_sow.json (Phase 4 assembled output)

# Legacy mode:
cd /tmp/sow_author_<execution_id>

# Key files (legacy):
# - Course_data.txt (extracted SQA data)
# - authored_sow.json (generated SOW)
# - sow_critic_result.json (unified critic feedback)
# - schema_validation_result.json (pydantic validation)
```

---

## API Reference

### IterativeSOWAuthor (v3.0 - Default)

**Class**: `src.iterative_sow_author.IterativeSOWAuthor`

#### Constructor

```python
IterativeSOWAuthor(
    mcp_config_path: str = ".mcp.json",
    persist_workspace: bool = True,
    log_level: str = "INFO"
)
```

**Parameters**:
- `mcp_config_path` (str): Path to MCP configuration file
- `persist_workspace` (bool): If True, preserve workspace for debugging
- `log_level` (str): Logging level (DEBUG, INFO, WARNING, ERROR)

**Returns**: IterativeSOWAuthor instance

---

#### execute()

```python
async execute(courseId: str, version: str = "1") -> Dict[str, Any]
```

**Parameters**:
- `courseId` (str): Course identifier (must exist in default.courses)
- `version` (str): SOW version string (default: "1")

**Returns**: Dictionary with execution results

**Success Response**:
```python
{
    "success": True,
    "execution_id": "20260119_143045",
    "workspace_path": "/tmp/iterative_sow_20260119_143045",
    "appwrite_document_id": "68f616168886c3362749",
    "phases": {
        "outline": {"status": "completed", "lessons_planned": 15},
        "lessons": {"status": "completed", "generated": 15, "failed": 0},
        "metadata": {"status": "completed"},
        "assembly": {"status": "completed"}
    },
    "metrics": {
        "total_tokens": 65432,
        "total_cost_usd": 0.98,
        "execution_time_seconds": 185.3
    }
}
```

**Failure Response**:
```python
{
    "success": False,
    "execution_id": "20260119_143045",
    "error": "Lesson 5 validation failed after 3 retries",
    "workspace_path": "/tmp/iterative_sow_20260119_143045",
    "phases": {
        "outline": {"status": "completed", "lessons_planned": 15},
        "lessons": {"status": "failed", "generated": 4, "failed": 1}
    },
    "metrics": {...}
}
```

---

#### Workspace Files (Iterative Mode)

When `persist_workspace=True`, the workspace contains:

```
workspace/<execution_id>/
‚îú‚îÄ‚îÄ Course_outcomes.json         # Extracted SQA curriculum data from Appwrite
‚îú‚îÄ‚îÄ sow_research.md              # Optional: Web research notes (if conducted)
‚îú‚îÄ‚îÄ lesson_outline.json          # Phase 1: Lesson sequence outline
‚îú‚îÄ‚îÄ outline_critic_result.json   # Phase 1: Critic evaluation result
‚îú‚îÄ‚îÄ current_outline.json         # Context: Current lesson's outline entry
‚îú‚îÄ‚îÄ previous_lessons.json        # Context: Previously generated lessons
‚îú‚îÄ‚îÄ all_lessons.json             # Context: All lessons for metadata generation
‚îú‚îÄ‚îÄ lesson_01.json               # Phase 2: Lesson 1
‚îú‚îÄ‚îÄ lesson_01_critic.json        # Phase 2: Lesson 1 critic result
‚îú‚îÄ‚îÄ lesson_02.json               # Phase 2: Lesson 2
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ lesson_N.json                # Phase 2: Final lesson
‚îú‚îÄ‚îÄ metadata.json                # Phase 3: Course-level metadata
‚îú‚îÄ‚îÄ authored_sow.json            # Phase 4: Assembled SOW (pre-compression)
‚îî‚îÄ‚îÄ test_metrics.json            # Debug: Phase timing and validation results
```

**Note**: Workspaces are preserved under `claud_author_agent/workspace/` directory, not `/tmp/`.

---

### SOWAuthorClaudeAgent (Legacy)

**Class**: `src.sow_author_claude_client.SOWAuthorClaudeAgent`

#### Constructor

```python
SOWAuthorClaudeAgent(
    mcp_config_path: str = ".mcp.json",
    persist_workspace: bool = True,
    log_level: str = "INFO"
)
```

**Parameters**:
- `mcp_config_path` (str): Path to MCP configuration file
- `persist_workspace` (bool): If True, preserve workspace for debugging
- `log_level` (str): Logging level (DEBUG, INFO, WARNING, ERROR)

**Note**: Retry/cycle management is handled by agent-level `max_turns` configuration (default: 500)

**Returns**: SOWAuthorClaudeAgent instance

---

#### execute()

```python
async execute(courseId: str) -> Dict[str, Any]
```

**Parameters**:
- `courseId` (str): Course identifier (must exist in default.courses)

**Returns**: Dictionary with execution results

**Success Response**:
```python
{
    "success": True,
    "execution_id": "20251029_143045",
    "workspace_path": "/tmp/sow_author_20251029_143045",
    "appwrite_document_id": "68f616168886c3362749",
    "metrics": {
        "total_tokens": 85432,
        "total_cost_usd": 1.2456,
        "execution_time_seconds": 245.3,
        "subagent_metrics": {...}
    }
}
```

**Failure Response**:
```python
{
    "success": False,
    "execution_id": "20251029_143045",
    "error": "Schema validation failed after 3 retries",
    "workspace_path": "/tmp/sow_author_20251029_143045",
    "metrics": {
        "total_tokens": 45123,
        "total_cost_usd": 0.6234
    }
}
```

---

### CLI Functions

**Module**: `src.sow_author_cli`

#### load_input_from_json()

```python
def load_input_from_json(json_path: str) -> Dict[str, str]
```

Load input parameters from JSON file.

**Parameters**:
- `json_path` (str): Path to JSON input file

**Returns**: Dictionary with courseId

**Raises**:
- `FileNotFoundError`: If JSON file not found
- `ValueError`: If JSON invalid or missing courseId

---

#### interactive_input()

```python
def interactive_input() -> Dict[str, str]
```

Prompt user for courseId interactively.

**Returns**: Dictionary with courseId

**Raises**:
- `ValueError`: If courseId is empty

---

#### run_agent()

```python
async run_agent(
    courseId: str,
    mcp_config_path: str = ".mcp.json",
    persist_workspace: bool = True,
    log_level: str = "INFO"
) -> Dict[str, Any]
```

Run SOW Author agent with given parameters (CLI helper function).

**Parameters**: Same as SOWAuthorClaudeAgent constructor + courseId

**Returns**: Result dictionary from agent execution

---

## Examples

### Example 1: Basic CLI Usage

```bash
# Interactive mode
python -m src.sow_author_cli

# Output:
# ==================================================
# SOW Author - Interactive Input
# ==================================================
#
# Please provide the Course ID:
#
# Course ID (e.g., 'course_c84474'):
#   (Must exist in default.courses collection)
#   > course_c84474
#
# ==================================================
# SOW Author Claude Agent
# ==================================================
#
# Input Parameters:
#   Course ID:     course_c84474
#   ...
#
# ‚úÖ SOW AUTHORING COMPLETED SUCCESSFULLY!
# ==================================================
#
# Results:
#   Execution ID:     20251029_143045
#   Workspace Path:   /tmp/sow_author_20251029_143045
#   Document ID:      68f616168886c3362749
#
# Metrics:
#   Total Tokens:     85432
#   Total Cost (USD): $1.2456
```

---

### Example 2: Programmatic Usage

```python
import asyncio
from pathlib import Path
from src.sow_author_claude_client import SOWAuthorClaudeAgent

async def batch_generate_sows():
    """Generate SOWs for multiple courses."""

    course_ids = [
        "course_c84474",  # National 5 Mathematics
        "course_c75773",  # National 3 Physics
        "course_c91234"   # National 4 Biology
    ]

    agent = SOWAuthorClaudeAgent(
        mcp_config_path=".mcp.json",
        persist_workspace=True,
        log_level="INFO"
    )

    results = []
    total_cost = 0.0

    for course_id in course_ids:
        print(f"\n{'='*60}")
        print(f"Generating SOW for: {course_id}")
        print(f"{'='*60}\n")

        result = await agent.execute(courseId=course_id)
        results.append(result)

        if result["success"]:
            cost = result["metrics"]["total_cost_usd"]
            total_cost += cost
            print(f"‚úÖ Success! Cost: ${cost:.4f}")
        else:
            print(f"‚ùå Failed: {result['error']}")

    print(f"\n{'='*60}")
    print(f"Batch Summary:")
    print(f"  Total SOWs: {len(course_ids)}")
    print(f"  Successful: {sum(1 for r in results if r['success'])}")
    print(f"  Failed: {sum(1 for r in results if not r['success'])}")
    print(f"  Total Cost: ${total_cost:.4f}")
    print(f"{'='*60}\n")

    return results

# Run batch generation
results = asyncio.run(batch_generate_sows())
```

---

### Example 3: Custom Validation

```python
import asyncio
import json
from pathlib import Path
from src.sow_author_claude_client import SOWAuthorClaudeAgent
from src.tools.sow_validator_tool import validate_sow_schema

async def generate_and_validate():
    """Generate SOW and run additional validation."""

    agent = SOWAuthorClaudeAgent()
    result = await agent.execute(courseId="course_c84474")

    if not result["success"]:
        print(f"‚ùå Generation failed: {result['error']}")
        return

    # Read generated SOW
    workspace = Path(result["workspace_path"])
    sow_path = workspace / "authored_sow.json"
    sow_json = sow_path.read_text()

    # Run standalone Pydantic validation
    validation_result = validate_sow_schema(sow_json)

    print(f"\n{'='*60}")
    print(f"Validation Results:")
    print(f"{'='*60}")
    print(json.dumps(validation_result, indent=2))

    if validation_result["valid"]:
        print(f"\n‚úÖ SOW is valid!")
        print(f"   Entries: {validation_result['stats']['total_entries']}")
        print(f"   Cards: {validation_result['stats']['total_cards']}")
    else:
        print(f"\n‚ùå Validation failed with {len(validation_result['errors'])} errors")
        for error in validation_result['errors'][:5]:
            print(f"   - {error['location']}: {error['message']}")

# Run
asyncio.run(generate_and_validate())
```

---

## Version History

### v3.0.1 (2026-01-20) - Phase Testing Complete
- ‚úÖ **All 4 phases comprehensively tested** with Applications of Mathematics Higher
- ‚úÖ Storage Bucket fallback for entries >100K chars (Appwrite Storage)
- ‚úÖ Entry trimming for size reduction (409K ‚Üí 92K chars after compression)
- ‚úÖ Critic loops with 5-dimension scoring (Coverage, Sequencing, Policy, Accessibility, Authenticity)
- ‚úÖ Simplified outline lesson types: `teach` and `mock_exam` only
- ‚úÖ Simplified card flow: 5 cards (starter, explainer, modelling, guided_practice, exit_ticket)
- ‚úÖ Test results documented with workspace preservation
- ‚úÖ Updated workspace file structure (Course_outcomes.json, critic results)

### v3.0 (2026-01-19) - Iterative Architecture
- ‚úÖ New iterative lesson-by-lesson generation mode (default)
- ‚úÖ 4-phase architecture: Outline ‚Üí Lessons (loop) ‚Üí Metadata ‚Üí Assembly
- ‚úÖ New `IterativeSOWAuthor` class using Claude Agent SDK
- ‚úÖ Six prompts: `outline_author`, `outline_critic`, `lesson_entry`, `lesson_critic`, `metadata_author`
- ‚úÖ Minimal JSON schemas for structured output (`src/utils/minimal_schemas.py`)
- ‚úÖ Pure Python assembler with cross-lesson validation
- ‚úÖ Better schema compliance via small scope (~4K tokens per lesson)
- ‚úÖ WebSearch/WebFetch per lesson for targeted research
- ‚úÖ CLI flags: `--iterative` (default), `--legacy`
- ‚úÖ Full DevOps pipeline integration

### v2.0 (2025-10-29) - Pydantic Optimization
- ‚úÖ Replaced 1265-line schema file with Pydantic models
- ‚úÖ Schema_critic now uses `mcp__validator__validate_sow_schema` tool
- ‚úÖ Token savings: ~13-16K per execution
- ‚úÖ Execution time: 30+ seconds ‚Üí 5-10 seconds
- ‚úÖ Simplified schema_critic prompt: 737 ‚Üí 322 lines

### v1.0 (2025-09-15) - Initial Release
- ‚úÖ Auto-fetching subject/level from Appwrite
- ‚úÖ Belt-and-braces validation strategy
- ‚úÖ On-demand WebSearch/WebFetch research
- ‚úÖ 3-subagent pipeline (author, unified_critic, schema_critic)
- ‚úÖ Cost tracking and metrics

---

## Support

For issues, questions, or feature requests:
- **GitHub Issues**: https://github.com/schoolofai/ScottishAILessons/issues
- **Documentation**: `claud_author_agent/docs/`
- **Example SOWs**: `claud_author_agent/example_sow.json`

---

**End of Guide**
