# SoW Author Prompt Enrichment Specification

## Status
**Draft** | Created: 2025-10-13 | Split from: `sow-enrichment-pipeline-spec.md`

## Overview

This spec defines **AI prompt changes** needed to support the post-processing enrichment pipeline. The goal is to remove trivial metadata generation from AI prompts, allowing the LLM to focus solely on pedagogical decision-making while the enrichment pipeline handles boilerplate fields.

**Related Spec**: `sow-seeding-enrichment-spec.md` (implementation of enrichment pipeline)

## Problem Statement

The current SoW Author prompts waste significant resources on trivial field generation:

1. **Token Waste**: ~25-30% of prompt tokens spent describing metadata field requirements ($id, version, status, timestamps, lessonTemplateRef)
2. **Cognitive Overhead**: LLM reasoning cycles consumed by non-pedagogical tasks
3. **Downstream Lookup Burden**: Lesson author receives codes like "AS1.1" and must lookup descriptions in Course_data.txt (lesson_author_prompts.py:1048, 1133)

**Impact**:
- 350-450 tokens wasted per SoW generation on metadata instructions
- Lesson author performs ~80-100 Course_data.txt lookups per course
- ~15% error rate when lesson author misinterprets assessment standard codes

## Goals

1. **Simplify AI Task**: Remove all trivial metadata field generation from prompts
2. **Focus on Pedagogy**: LLM generates only fields requiring educational judgment
3. **Enable Enrichment**: Produce minimal AI output that enrichment pipeline can enhance
4. **Maintain Quality**: Preserve all pedagogical decision-making capabilities

## Field Classification

### Fields to REMOVE from AI Prompts

These fields will be generated by the enrichment pipeline and should be **completely removed** from prompt instructions:

| Field | Generation Method | Rationale |
|-------|-------------------|-----------|
| `$id` | `f"csow_{courseId}"` or UUID | Trivial slug/UUID generation |
| `version` | Default to `1` | Simple integer, no AI needed |
| `status` | Default to `"draft"` | Enum default value |
| `createdAt` | System timestamp | No AI judgment required |
| `updatedAt` | System timestamp | No AI judgment required |
| `entries[].lessonTemplateRef` | `f"AUTO_TBD_{order}"` | String template from order |
| `entries[].coherence.unit` | Lookup from Course_data using outcomeRefs | Database query |

**Token Savings**: ~350-450 tokens per SoW generation

### Fields to KEEP in AI Prompts

These fields require **pedagogical judgment, creative synthesis, or strategic planning**:

#### SoW-Level Metadata
- `courseId` - Must match Course_data.txt (validated, not generated)
- `metadata.coherence.policy_notes` - Strategic calculator usage sequencing
- `metadata.coherence.sequencing_notes` - Curriculum flow rationale
- `metadata.accessibility_notes` - Accessibility strategies for course
- `metadata.engagement_notes` - Scottish context selection
- `metadata.weeks` - Teaching time planning
- `metadata.periods_per_week` - Scheduling strategy

#### Entry-Level Fields (Pedagogical Core)
- **`order`** - **CRITICAL**: Prerequisite relationships, skill progressions
- `label` - Teacher-facing lesson title
- `lesson_type` - Instructional role (teach, practice, assessment, etc.)
- `coherence.block_name` - Thematic grouping within unit
- `coherence.block_index` - Visual ordering aid (e.g., "2.1", "2.2")
- `coherence.prerequisites` - Dependency relationships
- `policy.calculator_section` - Calculator usage staging per lesson
- `policy.assessment_notes` - Assessment guidance
- `engagement_tags` - Scottish contexts (bus fares, NHS, etc.)
- `outcomeRefs` - SQA outcome codes
- **`assessmentStandardRefs`** - **KEEP AS CODES**: Enrichment adds descriptions
- `pedagogical_blocks` - Lesson structure (starter, guided practice, etc.)
- `accessibility_profile` - Per-lesson accessibility emphasis
- `estMinutes` - Duration estimation
- `notes` - Teacher guidance

**Why `assessmentStandardRefs` stays as codes**: The enrichment pipeline will transform this into enriched `assessmentStandards` array with full descriptions, eliminating downstream lookups.

## Prompt Changes Required

### 1. SOW_AGENT_PROMPT

**File**: `langgraph-author-agent/src/sow_author_prompts.py:3-360`

#### Remove from `<outputs>` section:
```xml
<!-- DELETE these lines -->
<outputs>
You MUST write these flat files (state["files"]["<name>"] = <json/string>):
- `authored_sow_json`: Final SoW (valid sow schema as in <schema_sow_with_field_descriptions> JSON object).
  * MUST include: $id, courseId, version, status, createdAt, updatedAt
  * MUST include: entries[].lessonTemplateRef for each entry
...
</outputs>
```

#### Update to:
```xml
<outputs>
You MUST write these flat files (state["files"]["<name>"] = <json/string>):
- `authored_sow_json`: SoW pedagogical content (enrichment pipeline will add metadata).
  * MUST include: courseId (for validation)
  * MUST include: entries[] with pedagogical content (order, label, lesson_type, etc.)
  * OPTIONAL: $id, version, status, timestamps (enrichment will add if missing)
  * DO NOT include: lessonTemplateRef (enrichment generates from order field)
...
</outputs>
```

#### Remove from `<schema_sow_with_field_descriptions>`:
```xml
<!-- Simplify schema documentation -->
{
  <!-- REMOVE instructions about generating these fields -->
  "$id": "csow_<slug_or_uuid>",  <!-- DELETE: "Generate with ID.unique() or slugify" -->
  "version": 1,                   <!-- DELETE: "Increment on republish" -->
  "status": "draft" | "published", <!-- DELETE: "Set based on approval state" -->
  ...
  "entries": [
    {
      "lessonTemplateRef": "AUTO_TBD_1", <!-- DELETE: "Use AUTO_TBD_{n} pattern" -->
```

#### Update to:
```xml
{
  "$id": "string, optional (enrichment generates)",
  "courseId": "string, REQUIRED - must match Course_data.txt",
  "version": "int, optional (enrichment defaults to 1)",
  "status": "string, optional (enrichment defaults to 'draft')",

  "metadata": {
    "coherence": {
      "policy_notes": ["REQUIRED: Strategic calculator staging"],
      "sequencing_notes": ["REQUIRED: Curriculum flow rationale"]
    },
    ...
  },

  "entries": [
    {
      "order": "int, REQUIRED - pedagogical sequencing decision",
      <!-- REMOVE lessonTemplateRef - enrichment generates -->
      "label": "string, REQUIRED - teacher-facing title",
      ...
    }
  ],

  <!-- REMOVE createdAt/updatedAt - enrichment generates -->
}
```

### 2. SOW_AUTHOR_SUBAGENT_PROMPT

**File**: `langgraph-author-agent/src/sow_author_prompts.py:618-763`

#### Update `<workflow>` section:
```xml
<!-- BEFORE -->
<workflow>
1. **Validate Research Pack** (FAIL-FAST)
2. **Validate Course Data** (FAIL-FAST)
3. **Read both files**
4. **Plan per-assessment-standard sequences**
5. **Draft a new SoW JSON** that aligns with the SoW data model schema:
   - Use official unit titles and codes from Course_data.txt
   - Generate $id using course slug
   - Set version to 1, status to 'draft'
   - Generate lessonTemplateRef as AUTO_TBD_{order}
   - Include createdAt/updatedAt timestamps
   ...
</workflow>
```

#### Update to:
```xml
<workflow>
1. **Validate Research Pack** (FAIL-FAST)
2. **Validate Course Data** (FAIL-FAST)
3. **Read both files**
4. **Plan per-assessment-standard sequences**
5. **Draft a new SoW JSON** focusing on pedagogical content:
   - Specify courseId from Course_data.txt (for validation)
   - CRITICAL: Set `order` field for each entry (prerequisite sequencing)
   - Use outcomeRefs and assessmentStandardRefs (codes only)
   - Focus on lesson_type, label, policy, engagement_tags, notes
   - Enrichment pipeline will add: $id, version, status, timestamps, lessonTemplateRef, coherence.unit
   ...
</workflow>
```

#### Update `<constraints>` section:
```xml
<!-- ADD -->
<constraints>
- **Focus on pedagogical content only** - metadata will be added by enrichment
- **CRITICAL: Always set `order` field** - this is required for lessonTemplateRef generation
- **Use codes for assessmentStandardRefs** - enrichment will add full descriptions
- **Do not generate**: $id, version, status, createdAt, updatedAt, lessonTemplateRef, coherence.unit
- **Do generate**: All fields requiring educational judgment
</constraints>
```

### 3. Critic Prompts (No Changes Needed)

The 5 critic prompts (Coverage, Sequencing, Policy, Accessibility, Authenticity) do NOT need changes. They critique pedagogical content and don't validate metadata fields.

**Rationale**: Critics evaluate lesson sequences, assessment coverage, accessibility strategies - all pedagogical concerns. They don't check $id format or timestamp validity.

## Assessment Standard Enrichment

### Current Schema (AI Output)
```json
{
  "entries": [
    {
      "assessmentStandardRefs": ["AS1.1", "AS1.2"]
    }
  ]
}
```

**Problem**: Lesson author must lookup descriptions in Course_data.txt, consuming tokens and risking errors.

### Enriched Schema (After Pipeline)
```json
{
  "entries": [
    {
      "assessmentStandardRefs": ["AS1.1", "AS1.2"],  // Legacy field preserved
      "assessmentStandards": [                        // NEW enriched field
        {
          "code": "AS1.1",
          "description": "Select and use appropriate notation and units for everyday situations",
          "outcome": "O1",
          "unit": "Applications of Mathematics: Manage Money and Data"
        },
        {
          "code": "AS1.2",
          "description": "Carry out calculations involving whole numbers, fractions, percentages, money, time and measurement",
          "outcome": "O1",
          "unit": "Applications of Mathematics: Manage Money and Data"
        }
      ]
    }
  ]
}
```

**Benefits**:
1. **Self-Contained**: Lesson author has all context inline
2. **Token Savings**: ~200-300 tokens per lesson (no lookup instructions)
3. **Error Reduction**: No misinterpretation of codes
4. **Backward Compatible**: Legacy field preserved for existing consumers

## Downstream Impact: Lesson Author Prompts

**File**: `langgraph-author-agent/src/lesson_author_prompts.py`

### Remove Lookup Instructions

**Lines to DELETE**:
- Line 59-60: Instructions to read Course_data.txt for outcome/standard descriptions
- Line 1048: *"REFERENCE the assessment standard being tested (from Course_data.txt)"*
- Line 1133: *"check Course_data.txt for assessment standard descriptions"*

### Update Input Schema

**BEFORE**:
```python
<inputs>
- `Course_data.txt`: Official SQA course structure (read for assessment standard descriptions)
- `sow_entry`: Single SoW entry with assessmentStandardRefs: ["AS1.1"]
</inputs>

<workflow>
1. Read Course_data.txt to lookup assessment standard descriptions
2. For code "AS1.1", find description in Course_data outcomes[]
3. Use description to inform card depth and rubric criteria
...
</workflow>
```

**AFTER**:
```python
<inputs>
- `sow_entry`: Single SoW entry with enriched assessmentStandards:
  [
    {
      "code": "AS1.1",
      "description": "Select and use appropriate notation...",
      "outcome": "O1",
      "unit": "Applications of Mathematics..."
    }
  ]
</inputs>

<workflow>
1. Read sow_entry.assessmentStandards[] for full context (no lookup needed)
2. Use description to inform card depth and rubric criteria
3. Design CFUs that directly assess the description requirements
...
</workflow>
```

**Token Savings**: ~200-300 tokens per lesson × ~80 lessons = ~16,000-24,000 tokens per course

## Implementation Checklist

### Phase 1: Update SoW Author Prompts (2 hours)
- [ ] Update `SOW_AGENT_PROMPT` `<outputs>` section
- [ ] Simplify `SOW_AGENT_PROMPT` schema documentation
- [ ] Update `SOW_AUTHOR_SUBAGENT_PROMPT` `<workflow>` section
- [ ] Add constraints about metadata generation to subagent prompt
- [ ] Remove timestamp/version/ID generation instructions throughout

### Phase 2: Update Lesson Author Prompts (1 hour)
- [ ] Remove Course_data.txt lookup instructions (lines 59-60)
- [ ] Remove assessment standard lookup references (lines 1048, 1133)
- [ ] Update input schema to show enriched assessmentStandards format
- [ ] Add examples using enriched format
- [ ] Update workflow steps to use inline descriptions

### Phase 3: Testing (1 hour)
- [ ] Test SoW generation with updated prompts (verify no metadata in output)
- [ ] Verify enrichment pipeline adds missing fields
- [ ] Test lesson generation with enriched SoW entries
- [ ] Measure token savings in both prompts
- [ ] Validate error rates with enriched data

## Success Criteria

### Prompt Simplification
- [ ] **Token Reduction**: ≥300 tokens saved in SoW author prompts
- [ ] **Token Reduction**: ≥200 tokens saved per lesson in lesson author prompts
- [ ] **Clarity Improvement**: 0 metadata generation instructions in prompts
- [ ] **Focus**: 100% of prompt content addresses pedagogical decisions

### Enrichment Integration
- [ ] **Backward Compatibility**: 100% of legacy fields (assessmentStandardRefs) preserved
- [ ] **Forward Enhancement**: 100% of entries have enriched assessmentStandards
- [ ] **Validation**: 0 broken references in enriched output
- [ ] **Performance**: No degradation in generation speed

### Quality Metrics
- [ ] **Lesson Author Lookups**: Reduced from ~80-100 to 0 per course
- [ ] **Lesson Author Errors**: ≤5% (reduced from ~15%)
- [ ] **SoW Quality**: No degradation in pedagogical quality

## Rollout Strategy

### Stage 1: SoW Prompts (Week 1)
1. Update SoW author prompts with metadata removal
2. Test with sample generations
3. Verify enrichment pipeline adds missing fields
4. Deploy to development environment

### Stage 2: Lesson Prompts (Week 2)
1. Update lesson author prompts with enriched format
2. Test lesson generation with enriched SoW entries
3. Measure token savings and error reduction
4. Deploy to development environment

### Stage 3: Validation (Week 3)
1. Generate complete course (SoW + lessons) with new prompts
2. Compare quality against legacy approach
3. Measure performance metrics
4. Gather feedback from test users

### Stage 4: Production (Week 4)
1. Deploy to production environment
2. Monitor metrics (tokens, errors, quality)
3. Document lessons learned
4. Update training materials

## Dependencies

### Prerequisites
- [ ] Enrichment pipeline implemented in seedAuthoredSOW.ts (see `sow-seeding-enrichment-spec.md`)
- [ ] course_outcomes collection populated with SQA data
- [ ] Validation that enriched output matches expected schema

### Coordination Required
- [ ] Test that minimal AI output + enrichment = complete SoW
- [ ] Verify lesson author can consume enriched format
- [ ] Update any frontend displays using assessment standard data

## Open Questions

1. **Prompt Validation**: Should prompts explicitly forbid metadata generation or just omit instructions?
   - *Recommendation*: Add explicit constraint "Do not generate: $id, version..." to prevent hallucination

2. **Partial Metadata**: What if AI generates some metadata fields despite instructions?
   - *Recommendation*: Enrichment overwrites all metadata fields (idempotent)

3. **Error Messaging**: Should AI output include validation errors if required fields missing?
   - *Recommendation*: Yes, fail with clear message if `order` field missing

4. **Backward Compatibility**: Should prompts support both old and new formats during transition?
   - *Recommendation*: No, clean cutover once enrichment pipeline deployed

---

## References

- **Implementation**: `sow-seeding-enrichment-spec.md`
- **SoW Author Prompts**: `langgraph-author-agent/src/sow_author_prompts.py`
- **Lesson Author Prompts**: `langgraph-author-agent/src/lesson_author_prompts.py`
- **Main Refactor Spec**: `langgraph-author-agent/tasks/sow_prompt-refactor.md`

---

**Document Owner**: AI Analysis | **Status**: Draft | **Last Updated**: 2025-10-13
